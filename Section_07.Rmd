---
title: "Section 07 Statistical sleuth: Intermediate"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

> Finding the most important predictor variables (of features) that explains major part of variance of the response variable is key to identify and build high performing models.

# ANOVA

Analysis of Variance (ANOVA) is a statistical technique, commonly used to studying differences between two or more group means. ANOVA test is centred on the different sources of variation in a typical variable. ANOVA in R primarily provides evidence of the existence of the mean equality between the groups. This statistical method is an extension of the t-test. It is used in a situation where the factor variable has more than one group. [^1]

[^1]: [R ANOVA Tutorial: One way & Two way](https://www.guru99.com/r-anova-tutorial.html)

## One-way ANOVA

There are many situations where you need to compare the mean between **multiple groups**. For instance, you may want to know the concentration of a certain chemical under 3 different treatments. The ANOVA test can tell if the 3 groups have similar concentration. 

To clarify if the data comes from the same population, you can perform a **one-way** analysis of variance (one-way ANOVA hereafter). This test, like any other statistical tests, gives evidence whether the *H0* hypothesis can be accepted or rejected. 

Hypothesis in one-way ANOVA test:

* Null hypothesis (H0): **The means between groups are identical**
* Altantive hypothesis: **At least, the mean of one group is different**

In other words, the H0 hypothesis implies that there is **not enough evidence** to prove the mean of the group (factor) are different from another. This test is similar to the *t-test*, although ANOVA test is recommended in situation with more than 2 groups. 

Let's use the [crop dataset](https://zhu-group.github.io/ese5023/download/crop.data.csv) to implement the one-way ANOVA test. The dataset contains 96 rows and 4 variables: 

* yield: Crop Yield 
* density: Type of density used: factor level: 1 and 2
* block: Type of block used: factor level: 1, 2, 3, and 4
* fertilizer: Type of fertilizer used: factor level: 1, 2, and 3

In R import the data:

```{r}
library(tidyr)
library(dplyr)
library(ggplot2)

# Read csv
crop_data <- read.csv("crop.data.csv", header=T)

# Change to factor type
crop_data_tbl <- as_tibble(crop_data) %>% 
  mutate(fertilizer = factor(fertilizer, ordered = TRUE)) %>% 
  mutate(block = factor(block, ordered = TRUE)) %>% 
  mutate(density = factor(density, ordered = TRUE))   

# Quick check
glimpse(crop_data_tbl)
```

Our objective is to test the following assumption:

* Null: There is no difference in crop yeild average between fertilizer group
* Altantive: The crop yeild average is different for at least one fertilizer group.

In other words, we want to know if there is a statistical difference between the mean of the crop yeild according to the type of fertilizer. 

First, we can check the level of the fertilizer with the following code:

```{r}
levels(crop_data_tbl$fertilizer)
```

We can compute the mean and standard deviation:
```{r}
crop_data_tbl %>%
  group_by(fertilizer) %>%
  summarise(
    count = n(),
    mean_yield = mean(yield, na.rm = TRUE),
    sd_yield = sd(yield, na.rm = TRUE)
  )
```

We can also graphically check if there is a difference between the distribution:

```{r}
ggplot(crop_data_tbl, aes(x = fertilizer, y = yield, fill = fertilizer)) +
  geom_boxplot() +
  theme_classic()
```

Finally, we run the one-way ANOVA test with the command `aov`. 

```{r}
anova_one_way <- aov(yield ~ fertilizer, data = crop_data_tbl)
summary(anova_one_way)
```

The p-value is lower than the usual threshold of 0.05. You are confident to say there is a statistical difference between the groups, indicated by the "*". 

## Pairwise comparison

The one-way ANOVA test does not inform which group has a different mean. Instead, you can perform a *Tukey test* with the function `TukeyHSD()`.

```{r}
TukeyHSD(anova_one_way)
```

## Two-way ANOVA

A two-way ANOVA test adds another group variable to the formula. It is identical to the one-way ANOVA test. Hypothesis in two-way ANOVA test:

* Null: The means are equal for both variables (i.e., factor variable)
* Altantive: The means are different for both variables

Let's add `block` variable to the model. This variable indicates the blocking method used in the land. Now we are interested to see if there is a statistical dependence between the fertilizer type (`fertilizer`) and blocking method (`block`).

```{r}
anova_two_way <- aov(yield ~ fertilizer + block, data = crop_data_tbl)
summary(anova_two_way)
```

We can conclude that both fertilizer type (`fertilizer`) and blocking method (`block`) are statistically different from 0. We can reject the Null hypothesis and confirm that changing the fertilizer type **or** the blocking method impact the crop yeilds. 

# Multiple Linear Regression

We have used the simple linear regression model in the last section, where we assume a linear relationship between a *response variable* and a single *predictor variable*.
However, it is rarely the case that a dataset will have a single predictor variable. 
It is also rarely the case that a response variable will only depend on a single variable.
So in this chapter, we will extend our current linear model to allow a response to depend on **multiple** predictors.

Let's use the `airquality` dataset from the `datasets` package that comes with R. The dataset contains daily ozone concentration, solar radiation, wind speed, and temperature in New York City from May to September. Load the `airquality` by:

```{r}
library(datasets)
library(stats)
data(airquality)
```

Check the data by 
```{r}
str(airquality)
```

Now we will focus on using three variables, `Solar.R`, `Wind`, and `Temp`, as predictor variables. That is, we would like to model the ozone contraction as a function
of solar radiation, wind speed, and temperature. To do so, we will define the following
linear model:

$$y_{i}  = \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x_{i2} + \beta_{3}x_{i3} + \epsilon_{i},  i = 1,2,3,...,n$$

where $\epsilon_{i} ~ N(0, \sigma^{2})$.  In this notation we will define:

* $x_{i1}$ as the solar radiation (`Solar.R`) of the *i*th day.
* $x_{i2}$ as the wind speed (`Wind`) of the *i*th day.
* $x_{i3}$ as the temperature (`Temp`) of the *i*th day.

The data points ($x_{i1}, x_{i2}, x_{i3}, y_{i}$) now exist in 4-dimensional space, so instead of fitting a line as we did in the simple linear regression model to the data, we will fit a 4-dimensional space. 

Before the linear regression model can be applied, one must verify multiple factors and make sure assumptions are met. Most of all one must make sure *linearity* exists between the variables in the dataset. One of the fastest ways to check the linearity is by using scatter plots.

```{r}
# plotting the data to determine the linearity
plot(airquality, main="Matrix Scatterplot")
```

From the above scatter plot we can determine the variables `Ozone`, `Solar.R`, `Wind`, and `Temp` in the database `airquality` are in linearity.

In R, the multiple linear regression is done as:

```{r}
model <- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality)
coef(model)
```

$$y_{i}  = -64.34 + -3.33x_{i1} + 1.65x_{i2} + 0.060x_{i3}$$
Here we have fitted our model using `lm()`, however we have introduced a new syntactical element. The formula `Ozone ~ Wind + Temp + Solar.R` reads: “model the response variable `Ozone` as a linear function of `Wind`, `Temp`, and `Solar.R`”. That is, it will estimate an intercept, as well as slope coefficients for `Wind`, `Temp`, and `Solar.R`. We then extract these as we have done before using `coef()`.

The model can be futher explored with the `summary()` function:
```{r}
summary(model)
```

In the above model, we assume that all the predictor variables are **independent** from each other. However, such assumption is falwed as we can see from the Matrix Scatterplot that `Temp` corrlates with `Wind`.

```{r}
cor(airquality$Wind, airquality$Temp)
```

Such **interaction variables** introduce an additional level of regression analysis by allowing researchers to explore the synergistic effects of combined predictors. In R,  you can interact two variables by using a colon `:`.

```{r}
model2 <- lm(Ozone ~ Wind + Temp + Solar.R + Wind:Temp, data = airquality)
summary(model2)
```

Indeed the interaction term has a signiicant slope, so you may want to include this term in your linear model.

***

# Overfitting in Regression Model

Overfitting a regression model occurs when you **attempt to estimate too many parameters from a sample that is too small**. Regression analysis uses one sample to estimate the values of the coefficients for all of the terms in the equation. The sample size limits the number of terms that you can safely include before you begin to overfit the model. 

Larger sample sizes allow you to specify more complex models. For trustworthy results, your sample size must be **large enough** to support the level of complexity that is required by your research question. If your sample size isn’t large enough, you won’t be able to fit a model that adequately approximates the true model for your response variable. You won’t be able to trust the results. 

You must have a sufficient number of observations for each term in a regression model. Simulation studies show that a good rule of thumb is to have **10-15 observations per term** in multiple linear regression. For example, if your model contains two predictors and the interaction term, you’ll need 30-45 observations. However, if the effect size is small or there is high multicollinearity, you may need more observations per term. [^1]

[^1]: [The Danger of Overfitting Regression Models](https://blog.minitab.com/blog/adventures-in-statistics-2/the-danger-of-overfitting-regression-models)

Let's demonstrate the idea by adding more fake variables to the linear model.

```{r}
N              <- dim(airquality)[1]
Fake           <- array(dim=c(N,10))
airquality_new <- airquality

# Generate some fake observations
for(i in 1:10){
  Fake[,i]     <- rnorm(N, 0, 1)
}

# Attach to the original data
airquality_new <- cbind(airquality,Fake)
airquality_new <- airquality_new[1:15,]

# Change column names
colnames(airquality_new)[7:16] = paste("Fake",seq(1,10),sep="")

# Fit linear model
fit <- lm(Ozone ~ Fake1 + Fake2 + Fake3 + Fake4 + Fake5 + 
            Fake6 + Fake7 + Fake8 + Fake9 + Fake10, data = airquality_new)
summary(fit)

```

You can see the high $R^{2}$ doesn't make any senses, as the linear model has no any valid observations.  


# Variable Selection 

We use Boston Housing Data as an illustrative example. Boston housing data is a built-in dataset in `MASS` package, so you do not need to download externally. Package `MASS` comes with R when you installed R, so **no need** to use `install.packages(MASS)` to download and install, but you do need to load this package. [^2]

[^2]: [Regression and Variable Selection](https://xiaoruizhu.github.io/Data-Mining-R/lecture/3_LinearReg.html#compare-model-fit-manually)

```{r}
library(MASS)
data(Boston); #this data is in MASS package
str(Boston)
```

The original data are 506 observations on 14 variables, `medv` being the response variable *y*.

We usually splitting our data to training and testing samples. In this case, we sample 90% of the original data and use it as the *training set*. The remaining 10% is used as *test set*. The regression model will be built on the training set and future performance of your model will be evaluated with the test set.

```{r}
sample_index <- sample(nrow(Boston),nrow(Boston)*0.90)
Boston_train <- Boston[sample_index,]
Boston_test  <- Boston[-sample_index,]
```

What we want to do next is to build a **best** model with training data. Let's begin with including all *x* varables in the model:

```{r, eval=F}
model_1 <- lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat, data=Boston_train)
```

To include all variables in the model, you can write the statement this simpler way.

```{r}
model_1 <- lm(medv ~ ., data=Boston_train)
summary(model_1)
```

## Best subset regression

You can use the `leaps` package to get the best subset regression.

```{r}
#install.packages('leaps')
library(leaps)
subset_result <- regsubsets(medv ~ ., data=Boston_train, nbest=2, nvmax = 14)
plot(subset_result, scale="bic")
```

Here `nbest` is number of subsets of each size to record, we use `2` so that each time R compares `2` varibales. `nvmax` is the maximum size of subsets to examine, this is set to `14` to conisder all the varibales in the `Boston_train` dataset.

Each row represents a model. Black indicates that a variable is included in the model, while white indicates that it is not. The argument `scale = ""` can be `Cp`, `adjr2`, `r2`, or `bic`.

* AIC stands for (Akaike’s Information Criteria), a metric developped by the Japanese Statistician, Hirotugu Akaike, 1970. The basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. **The lower the AIC, the better the model**. 

* AICc is a version of AIC corrected for small sample sizes.

* BIC (or Bayesian information criteria) is a variant of AIC with a stronger penalty for including additional variables to the model.

* Mallows Cp: A variant of AIC developed by Colin Mallows.

What is the problem with best subset regression? If there are *n* independent variables, the number of possible nonempty subsets is $2^{n}-1$. If you try a best subset regression with more than 50 variables, you might need to wait for your entire life to get the result.

## Variable Selection Using AIC

To perform the forward/backward/stepwise regression in R, we need to define the starting points:

```{r}
nullmodel=lm(medv ~ 1, data=Boston_train)
fullmodel=lm(medv ~ ., data=Boston_train)
```
nullmodel is the model with **no** varaible in it, while fullmodel is the model with **every** variable in it.

```{r}
# Backward Elimination
model_step_b <- step(fullmodel,direction='backward')
# Forward Selection
model_step_f <- step(nullmodel, scope=list(lower=nullmodel, upper=fullmodel), direction='forward')
# Stepwise Selection 
model_step_s <- step(nullmodel, scope=list(lower=nullmodel, upper=fullmodel), direction='both')
```

# In-class exercises

## Exercise #1

Please go over the notes once again, make sure you understand the scripts.

## Exercise #2

2.1 Using the crop yeild data, graphically check crop yeilds in different soil density (`density`) groups; and conduct a one-way ANOVA test to show whether there is difference in crop yeild average between soil density type (`density`) group.

2.2 Conduct a two-way ANOVA test to show whether changing the soil density type **or** the blocking method impact the crop yeilds. 

## Exercise #3

In this exercise, we consider CPU performance from different types. Load the data `cpus` in the `MASS` package by:
```{r}
library(MASS)
data(cpus)
```

3.1 Fit a best subset regression between predictor variable `perf` and response variables `syct`, `mmin`, `mmax`, `cach`, `chmin`, `chmax`. Try `bic` and `adjr2` in as the `scale` argument.

3.2 Using Forward, Backward, and Stepwise Regression method for variable selection.

# Further reading

* [Applied Statistics with R](https://daviddalpiaz.github.io/appliedstats/applied_statistics.pdf)

* [Introduction to Data Exploration and Analysis with R](https://bookdown.org/mikemahoney218/LectureBook/basic-statistics-using-r.html)

* [r-statistics](https://r-statistics.co/)

