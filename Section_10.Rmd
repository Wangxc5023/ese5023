---
title: "Section 10 Python environments and parallel computing with `dask`"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

> "Your code is the product of your environment." - Robert Sandor

*** 

# Prerequisites

In this section, we will use daily data from the [AVISO](https://www.aviso.altimetry.fr/en/home.html) sea-surface height satellite altimetry dataset. Download the file (`aviso_2015.zip`, ~ `390 MB`) [from here](), unzip it, you will have a folder (`aviso_2015`) containing `365` nc files. Move the folder to your working directory.

***

# `Python` environments

`Python` and nearly all of the software packages in the scientific `Python` ecosystem are **open-source**. They are maintained and developed by a community of scientists and programmers, some of whose work is supported by universities, non-profits, and for-profit corporations. Because there is no single authority that controls all of these packages, coordinating the compatibility between these different packages and their multiple versions used to be a nightmare.

Sometimes when we create software, the software needs to run on a **specific version** of `Python` because our software expects a certain behavior that is present in older versions but changes in newer versions. Likewise, we may need to use specific versions of the libraries for similar reasons. But we may have many projects on our computer, perhaps a project that runs on with `xarray` version 0.1 and `Python 2.7` and even a more modern project that runs on `xarray` version 0.8 and `Python 3.8`. If we try running both at once on `Python 2.7` or `Python 3.8`, one of them may break because some of the code that runs on `Python 2` doesn’t run on `Python 3` or vice versa. This is where virtual **environments** become useful.

Virtual environments keep these dependencies in separate "sandboxes" so you can switch between both applications easily and get them running. `Python` environments take care of most of the dependencies and modules for us, but we will have to get our hands dirty in `Python` and deal with the environments. 

In this section, we will learn how to use `conda` to manage a `Python` environment ("sandbox") we play in.

<img src="figs/sandbox.png" alt="drawing" width="600"/>  

[Figure source](https://miro.medium.com/max/544/1*IwZIB3XSOyfAf3DXK5jmEA.png)


## Creating a `Python` environment

`Python` coupled with a package manager provides a way to make isolated, reproducible environments where you have fine-tuned control overall packages and configuration. From now on, you are highly recommended to work within an environment, rather than the "default" environment. 

To see all the environments on your system, type the following in `Anaconda Powershell Prompt`:

```{bash, eval=F}
conda info --envs
```

This command should display the current environments, with `*` marking the environment you are in.

To quickly create an environment (called `cper`, standing for *Computing and Programming for Environmental Research*) using `conda`, type the following in `Anaconda Powershell Prompt`:

```{bash, eval=F}
conda create --name cper python=3.8.8
```

In this command, the `python=3.8.8` portion specifies which version of `Python` you want to set up the environment in; you can change the version to whatever suits your needs. Here `--name` sets the name of the environment. In other snippets you see online, you may see `--n` instead of `--name`; they mean exactly the same thing. 

Now display the current environments with `conda info --envs`, you will something like this:

```{bash, eval=F}
base                   * C:\Users\86133\anaconda3
cper                     C:\Users\86133\anaconda3\envs\cper
```

## Using a `Python` environment

After confirming that you created the environment, let's now actually use it. We can accomplish this by typing:

```{bash, eval=F}
conda activate cper
```

At this point, your terminal prompt should look something like this:

```{bash, eval=F}
(cper) PS C:\Users\86133>
```

Now display the current environments with `conda info --envs`, you will something like this:

```{bash, eval=F}
base                     C:\Users\86133\anaconda3
cper                   * C:\Users\86133\anaconda3\envs\cper
```

The `*` indicates that we are in the `cper` environment.

To get a complete summary of all the packages installed in an environment, the channel they were installed from, and their full version info, type:

```{bash, eval=F}
conda list
```

To stop using the `cper` environment, type in:

```{bash, eval=F}
conda deactivate cper
```

If you would like to get rid of the entire `cper` environment, type in:

```{bash, eval=F}
conda remove --name cper --all
```

The `--all` flag is to remove all packages from the environment and is necessary to completely clean the environment.

For extensive documentation on using environments, please see the [conda documentation](https://conda.io/projects/conda/en/latest/user-guide/concepts/environments.html). 

## Installing modules under a `Python` environment

Once you have a basic `Python` environment, you can easily add or remove modules using the `conda` command-line utility. `Conda` was created to help manage the complex dependencies and pre-compiled binary libraries that are necessary in scientific python. If you set up your python environment using the `Anaconda` Python Distribution or with `miniconda`, you should already have the `conda` command available on the command line. With it, you can easily install modules from an official, curated set of packages which are built and tested for a number of different system configurations on Linux, Windows, and macOS.

To install a module (say `XXXX`), first activate the environment (say `cper`) under which you would like to install it:

```{bash, eval=F}
conda activate cper
```

Then type:

```{bash, eval=F}
conda install XXXX
```

Additionally, there is a community-maintained collection of packages/recipes called [conda forge](https://conda-forge.org/) which is accessible through `conda` as a special "channel":

```{bash, eval=F}
conda install -c conda-forge XXXX
```

While `conda` allows you to install almost any science-related package, there may be other general-use python packages you wish to you that are not available via `conda`. For these, you can use an alternative installation method.

Outside of the scientific python community, the most common way to install packages is to search for them on the [official `PyPI` index](https://pypi.org/). Once you've found the package you want to install (you may have also just found it on Github or elsewhere), you use the `pip` command from the command line:

```{bash, eval=F}
pip install XXXX
```

If you can't find a package on either `PyPI` or `conda-forge`, you can always install it directly from the source code. If the package is on `github`, `pip` already has an alias to do this for you:

```{bash, eval=F}
pip install git+https://github.com/YYYY/XXXX.git
```

Where `YYYY` is the username of github.

Now let's install some modules under the `cper` environment. Make sure you have activated the `cper` environment; otherwise, modules will be installed under the `base` environment.

First, we will install modules via `conda`:

```{bash, eval=F}
conda install numpy pandas matplotlib netCDF4 xarray notebook cartopy dask python-graphviz
```

Later, we use `pip` to install a module via a different channel:

```{bash, eval=F}
pip install nc-time-axis
```

Now use `conda list` to verify that we have successfully installed the above modules.

## Using the environment file

As you type `conda list`, you get a complete summary of all the modules installed in your environment, the channel they were installed from, and their full version info. Using this info, you are able to create an environment file in [`YAML` syntax](https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html) (YAML Ain’t Markup Language) which documents the exact contents of your environment. 

To export your current environment (`cper`) to a `yml` file (`my_environment.yml`):

```{bash, eval=F}
conda env export --name cper > my_environment.yml
```

The `>` indicates that the output is writing to a file called `my_environment.yml`. If there were any contents in `my_environment.yml` before this command, they would be overwritten. Now in your working directory, a file `my_environment.yml` was just created, open it, you will see a list of configurations of the `cper` environment. 

The environment file (`my_environment.yml`) is very useful, as it allows creating an **identical** environment. You can use different environment files for different projects, or share an environment file with someone to grantee **reproducible research**.

To create an environment through existing environment file (say `ZZZZ.yml`), go to the folder that contains this environment file, type:

```{bash, eval=F}
conda env create -f ZZZZ.yml
```

***

# `Dask` for parallel computing

In past lectures, we learned how to use `numpy`, `pandas`, and `xarray` to analyze various types of data. In this section, we address an increasingly common problem: what happens if the data we wish to analyze is "big data".

A good threshold for when data becomes difficult to deal with is when the volume of data exceeds your computer's RAM (Random-access memory). Most modern laptops have between `2 GB` and `16 GB` of RAM. High-end workstations and servers can have `1 TB` or RAM or more. If the dataset you are trying to analyze can't fit in your computer's memory, some special care is required to carry out the analysis. 

The next threshold of difficulty is when the data can't fit on your hard drive. Most modern laptops have between `100 GB` and `4 TB` of storage space on the hard drive. If you can't fit your dataset on your internal hard drive, you can buy an external hard drive. However, at that point you are better off using a high-end server, HPC system, or cloud-based storage for your dataset. Once you have many `TB` of data to analyze, you are definitely in the realm of "big data". 

## Introduction to `dask`

`Dask` is a tool that helps us easily extend our familiar `Python` data analysis tools to medium and big data, i.e. dataset that can't fit in our computer's RAM. In many cases, `dask` also allows us to speed up our analysis by using multiple CPU cores. `Dask` can help us work more efficiently on our laptop, and it can also help us **scale up** our analysis on HPC and cloud platforms. Most importantly, `dask` is almost invisible to the user, meaning that you can focus on your science, rather than the details of parallel computing.

`Dask` provides collections for big data and a scheduler for parallel computing. In this section, we will learn the basics of using `dask` through examples. For more about `dask`, check [The Dask Documentation](https://docs.dask.org/en/latest/) and [The Dask Github Site](https://github.com/dask/dask). For other modules enabling parallel computing, check [Parallel Processing and Multiprocessing in Python](https://wiki.python.org/moin/ParallelProcessing).


## Making a local client

`Dask` has several ways of executing code in parallel. We'll use the [distributed scheduler](http://distributed.dask.org/en/stable/) by creating a `dask.distributed.Client`. For now, this will provide us with some nice diagnostics. 

Let's make a local client (`my_client`) with `4` workers (`1` thread per worker):

```{python, eval=F}
# Use the distributed scheduler to form a local client (cluster)
# 4 works, 1 thread (CPU) per work
my_client = Client(n_workers=4, threads_per_worker=1)

# Show information of the local client
my_client.cluster
```

Click the the Dashboard link, get yourself familiarized with it.

If you no longer use a client, make sure to close your client or stop this kernel:

```{python, eval=F}
my_client.close()
```

## Parallelizing code with `dask.delayed()`

In this sub-section we parallelize some simple code with `dask` and [`dask.delayed`](https://docs.dask.org/en/latest/delayed.html). Often, this is the only function that you will need to convert functions for use with `dask`. This is a simple way to use `dask` to parallelize existing codebases or build complex systems.

First let's make some toy functions, `fun1` and `fun2`, that sleep for a while to simulate work. We'll then time running these functions normally.

```{python, eval=F}
# Define two functions
def fun1(x):
    sleep(1)
    return x + 1

def fun2(x, y):
    sleep(1)
    return x + y
```

We then time the execution of this normal code using the `%%time` magic, which is a special function of the `Jupyter Notebook`.

```{python, eval=F}
%%time

# This takes three seconds to run because we call each
# function sequentially, one after the other

x = fun1(1)
y = fun1(2)
z = fun2(x, y)
```

Those two increment calls could be called in parallel, because they are totally **independent of one-another**.

We'll transform the `fun1` and `fun2` functions using the `dask.delayed()` function. When we call the **delayed version** by passing the arguments, exactly as before, the original function **isn't** actually called yet - which is why the cell execution finishes very quickly. **Instead, a delayed object is made, which keeps track of the function to call and the arguments to pass to it.**

This ran immediately, since nothing has really happened yet. To get the result, call `compute`. Notice that this runs faster than the original code.

```{python, eval=F}
%%time

# This actually runs our computation using a local cluster
z.compute()
```

Here, the `z` object is a *lazy* `Delayed` object, which holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another. We can evaluate the result with `.compute()` as above or we can visualize the task graph for this value with `.visualize()`. As you can see, such a lazy execution allows building up complex, large calculations symbolically before turning them over to the scheduler for execution. For more, check [Dask lazy execution](https://tutorial.dask.org/01x_lazy.html).

```{python, eval=F}
# Look at the task graph for z
z.visualize()
```

Notice that this includes the names of the functions from before, and the logical flow of the outputs of the `fun1` functions to the inputs of `fun2`.

## Parallelizing a `for` loop with `dask.delayed()`

`for` loops are one of the most common things that we want to parallelize. Use `dask.delayed `on `fun1` and `sum` to parallelize the computation below:

```{python, eval=F}
# Make a simple list
data = [1, 2, 3, 4]
```

```{python, eval=F}
%%time

# Loop element one by one
# Sequential code
results = []

for i in data:
    temp = fun1(i)
    results.append(temp)

total = sum(results)
```

A parallel version with `dask.delayed()` would be:

```{python, eval=F}
%%time

# Parallel code 
results = []

for i in data:
    temp = delayed(fun1)(i)
    results.append(temp)
    
total = delayed(sum)(results)

# Let's see what type of thing total is
print("Before computing:", total)

# Compute
result = total.compute()

# After it's computed
print("After computing :", result)  
```

Visualize the task graph for this:

```{python, eval=F}
# Look at the task graph for total
total.visualize()
```

Now enlarge `data` to a larger list, e.g, `[1, 2, 3, 4, 5, 6, 7, 8]`. Check what would happen to the running time of `total`? Can you figure out why?

## Combing `dask` and `xarray`

`Xarray` can automatically wrap its data in [`dask` arrays](https://tutorial.dask.org/03_array.html). This capability turns `xarray` into an extremely powerful tool for Big Data earth science. 

To illustrate this in action, we will use a fairly large dataset to analyze. This file contains `1` year of daily data from the [AVISO](https://www.aviso.altimetry.fr/en/home.html) sea-surface height satellite altimetry dataset. 

Let's load the first file as a regular `xarray` dataset:

```{python, eval=F}
# Load the first file with xarray
ds_first = xr.open_dataset('aviso_2015/dt_global_allsat_madt_h_20150101_20150914.nc')

# Check the data
ds_first
```

This one file is about `8 MB`. So `365` of them will be nearly `3 GB`. If we had downloaded all `25` years of data, it would be `73 GB`!. This is a good example of "medium data".

With [`open_mfdataset`](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html), we can easily open all the netcdf files into one Dataset object. 

```{python, eval=F}
# Use open_mfdataset to load all the nc files
ds = xr.open_mfdataset('aviso_2015/*.nc')

# Check data object
# Notice that the values are not displayed
ds
```

Note that the values are not displayed, since that would trigger computation.

```{python, eval=F}
# Get sea surface height
ssh = ds.adt

# Check the data, this is a dask array
ssh
```

Try the following code, see what happens:

```{python, eval=F}
# Compute annual mean ssh
ssh_2015_mean = ssh.mean(dim='time')
```

To actually compute the data:

```{python, eval=F}
# Compute annual mean ssh
ssh_2015_mean.load()

# Plot annual mean
ssh_2015_mean.plot()
```

Finally, close the client (local cluster):

```{python, eval=F}
# Close the client (local cluster)
my_client.close()
```

***

# `Python` next?

So far we have covered the basics of `Python` and fundamental modules to make plots and analyze data. Looking forward, the learning curve is still steep, be patient as it takes time. Use Google and [stackoverflow](https://stackoverflow.com/) without hesitation, as you will find 9 out 10 times your question/issue has been reported and solved already. Have fun!

***

*The notes are modified from the excellent [Dask for Parallel Computing and Big Data](https://rabernat.github.io/research_computing_2018/dask-for-parallel-computing-and-big-data.html), [Python Environments](https://rabernat.github.io/research_computing_2018/python-environments.html), and [Getting started with Python environments (using Conda)](https://towardsdatascience.com/getting-started-with-python-environments-using-conda-32e9f2779307), all available freely online.*

***

# In-class exercises

## Exercise #1

**1.1** Create a `conda` environment called `test` with `Python 3.6` installed.

**1.2** Install a module under the `test` environment.

**1.3** Export `test` environment configuration to an environment file, and share it with your deskmate.

**1.4** Create a new `conda` environment called `test_mate` using your deskmate's environment file.

**1.5** Activate the `test_mate` environment, and verify it contains the module your deskmate specified in his/her environment file.

**1.6** Deactivate both `test` and `test_mate` environments.

**1.7** Remove both `test` and `test_mate` environments, along with their modules.

## Exercise #2

Compute and plot the variance in daily ssh anomalies with `dask`.

***

# Further reading

+ [Conda Cheat sheet](https://conda.io/projects/conda/en/latest/_downloads/843d9e0198f2a193a3484886fa28163c/conda-cheatsheet.pdf)
+ [Getting started with conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html)
+ [`Dask` tutorial](https://github.com/dask/dask-tutorial)
+ [A Data Scientist’s Guide to Lazy Evaluation with `Dask`](https://saturncloud.io/blog/a-data-scientist-s-guide-to-lazy-evaluation-with-dask/)
+ [Intermediate Python](https://book.pythontips.com/en/latest/)
