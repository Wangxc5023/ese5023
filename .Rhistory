msg              <- sprintf(paste0(as.character(Sys.time()), " ", text, "\n"), ...)
cat(msg)
write.socket(log.socket, msg)
}
#---> Define the daily merge function
Daily_merge        <- function(iday){
date               <- paste(substr(Date_list[iday],1,4),substr(Date_list[iday],6,7),
substr(Date_list[iday],9,10),sep="")
# Daily merge file
# 3i8,6f8.1,19f15.6,4E15.6
# ID,j,i,TimeUTC(1:6,j),CornerLat(i,j),CornerLat(i,j+1),  &
# CornerLat(i+1,j),CornerLat(i+1,j+1),Lat(i,j),           &
# CornerLon(i,j),CornerLon(i,j+1),CornerLon(i+1,j),       &
# CornerLon(i+1,j+1),Lon(i,j),SAA(i,j),SZA(i,j),VAA(i,j), &
# VZA(i,j),AMFCF(i,j),AMFCP(i,j),AMF(i,j),AMFg(i,j),      &
# Albe(i,j),Column(i,j),ColumnDest(i,j),                  &
# ColumnUnc(i,j),RefCorVCD(i,j)
merge_file         <- paste(merge_dir,"merge_",date,"_new",sep="")
# MERRA-2 daily file
nc_file1           <- paste(MERRA2_dir,"MERRA2_300.tavg1_2d_flx_Nx.",date,".nc4.nc",sep="")
nc_file2           <- paste(MERRA2_dir,"MERRA2_400.tavg1_2d_flx_Nx.",date,".nc4.nc",sep="")
if(file.exists(nc_file1)){
nc_file          <- nc_file1
}
if(file.exists(nc_file2)){
nc_file          <- nc_file2
}
# Check both files
if( file.exists(merge_file) && file.exists(nc_file) ){
#print(paste("  - Process:",date))
# Read  merge file
merge_data       <- read.table(merge_file)
Nlines           <- dim(merge_data)[1]
# Read MERRA-2 daily file
ex.nc	           <- open.nc(nc_file)
MERRA2_Lat       <- var.get.nc(ex.nc,"lat")
MERRA2_Lon       <- var.get.nc(ex.nc,"lon")
MERRA2_TLML      <- var.get.nc(ex.nc,"TLML") # surface_air_temperature [K]
close.nc(ex.nc)
# Define arries
Pixel_TLML       <- array(NA,dim=c(Nlines))
Pixel_Group      <- array(NA,dim=c(Nlines))
}else{
print(paste("!ERROR, Check files at:",date))
next
}
# Compute GC-informed surface number desnity from OMI
for(line in 1:Nlines){
#if( (line - (line%/%1000)*1000) == 0 ){
#  print(line/Nlines*100)
#}
# Get the row of CCD
Pixel_Row               <- as.numeric(merge_data[line,3])
# Only use rows that still vaild at the 2019
if( is.na(match(Pixel_Row,Rows_valid)) ){
# Pixel has a row that is not in Rows_valid
Pixel_TLML[line]      <- NA
Pixel_Group[line]     <- NA # Out of range
}else{
Pixel_LatCenter       <- as.numeric(merge_data[line,14])
Pixel_LonCenter       <- as.numeric(merge_data[line,19])
Pixel_LatConcers      <- as.numeric(merge_data[line,10:13])
Pixel_LonConcers      <- as.numeric(merge_data[line,15:18])
Pixel_Hour            <- as.numeric(merge_data[line,7])
# Skip bad pixels
if( is.character(c(Pixel_LatCenter,Pixel_LonCenter,Pixel_LatConcers,Pixel_LonConcers,Pixel_Hour)) ){
Pixel_TLML[line]    <- NA
Pixel_Group[line]   <- NA # Out of range
}else{
row_MERRA2          <- which(abs(Pixel_LatCenter-MERRA2_Lat)==min(abs(Pixel_LatCenter-MERRA2_Lat)))[1]
col_MERRA2          <- which(abs(Pixel_LonCenter-MERRA2_Lon)==min(abs(Pixel_LonCenter-MERRA2_Lon)))[1]
hour_MERRA2         <- Pixel_Hour + 1
Pixel_TLML[line]    <- MERRA2_TLML[col_MERRA2,row_MERRA2,hour_MERRA2]    # [K]
if( Pixel_TLML[line] >= Temp_range[2] || Pixel_TLML[line] <= Temp_range[1] ){
Pixel_Group[line] <- NA # Out of range
}else{
Pixel_Group[line] <- which(abs(Pixel_TLML[line]-Temp_groups)==min(abs(Pixel_TLML[line]-Temp_groups)))[1]
}
}
}
}
#---> Now loop Temp groups
for(igroup in 1:NGroups){
print(igroup)
group_list           <- which(Pixel_Group==igroup)
if(length(group_list)>0){
merge_data_new     <- cbind(merge_data[group_list,c(1:19,31:32)],Pixel_TLML[group_list])
#---> Format the arraies
Data_temp          <- data.frame( format(merge_data_new, digits = 8, scientific = TRUE) )
#---> Save the data to file
Output_Name        <- paste(merge_T_dir,"merge_group_",I2C(igroup),"_",date,"_",
"T_",as.character(Temp_groups[igroup]),sep="")
write.fwf(x=Data_temp,file=Output_Name, width=rep(16,22),colnames=FALSE)
}
}
#---> Clean up
rm(merge_data,merge_data_new,Data_temp,Pixel_TLML,Pixel_Group,MERRA2_TLML)
} # End of daily loop
#---> Allocate the threads for this job
Cluster            <- makeCluster(NThreads)
registerDoParallel(Cluster)
#---> Use foreach for parrallel computing
foreach(iday = 1:NDates, .inorder = FALSE, .packages=c('RNetCDF','gdata')) %dopar% {
# Call daily merge function
Daily_merge(iday)
# Process printed
Log("Processed %s | %d of %d | percent %7.3f", Date_list[iday], iday, NDates, iday/NDates*100)
}
#---> Disallocate the threads
stopCluster(Cluster)
#---> Libraries and source code
library(doParallel)
source("/home/lei/R/TOOLS.R")
#---> Merge file dir
merge_dir          <- "/data/OMI_v3.0/merge/"
merge_T_dir        <- "/data/OMI_v3.0/merge_T/"
MERRA2_dir         <- "/data/MERRA2/"
#---> Set temperature groups
Temp_range         <- c(278,323) # [K]
Temp_res           <- 0.5
Temp_groups        <- seq(Temp_range[1]+0.5*Temp_res,Temp_range[2]-0.5*Temp_res,by=Temp_res)
NGroups            <- length(Temp_groups)
#---> Set valid rows that are still valid at the 2019
# Deal with OMI row anomalies
Rows_valid         <- c(seq(1,20),seq(55,60))
#---> Set date range of interest
Date_range         <- c("2005-04-01", "2009-12-31")
Date_list          <- seq(as.Date(Date_range[1]),as.Date(Date_range[2]),by="days")
NDates             <- length(Date_list)
#---> Set number of threads for parallel computing
NThreads           <- 82
#---> Set log messages printed in the terminal
# Run "nc -l 4000" before this
log.socket         <- make.socket(port=4000)
Log                <- function(text, ...) {
msg              <- sprintf(paste0(as.character(Sys.time()), " ", text, "\n"), ...)
cat(msg)
write.socket(log.socket, msg)
}
#---> Define the daily merge function
Daily_merge        <- function(iday){
date               <- paste(substr(Date_list[iday],1,4),substr(Date_list[iday],6,7),
substr(Date_list[iday],9,10),sep="")
# Daily merge file
# 3i8,6f8.1,19f15.6,4E15.6
# ID,j,i,TimeUTC(1:6,j),CornerLat(i,j),CornerLat(i,j+1),  &
# CornerLat(i+1,j),CornerLat(i+1,j+1),Lat(i,j),           &
# CornerLon(i,j),CornerLon(i,j+1),CornerLon(i+1,j),       &
# CornerLon(i+1,j+1),Lon(i,j),SAA(i,j),SZA(i,j),VAA(i,j), &
# VZA(i,j),AMFCF(i,j),AMFCP(i,j),AMF(i,j),AMFg(i,j),      &
# Albe(i,j),Column(i,j),ColumnDest(i,j),                  &
# ColumnUnc(i,j),RefCorVCD(i,j)
merge_file         <- paste(merge_dir,"merge_",date,"_new",sep="")
# MERRA-2 daily file
nc_file1           <- paste(MERRA2_dir,"MERRA2_300.tavg1_2d_flx_Nx.",date,".nc4.nc",sep="")
nc_file2           <- paste(MERRA2_dir,"MERRA2_400.tavg1_2d_flx_Nx.",date,".nc4.nc",sep="")
if(file.exists(nc_file1)){
nc_file          <- nc_file1
}
if(file.exists(nc_file2)){
nc_file          <- nc_file2
}
# Check both files
if( file.exists(merge_file) && file.exists(nc_file) ){
#print(paste("  - Process:",date))
# Read  merge file
merge_data       <- read.table(merge_file)
Nlines           <- dim(merge_data)[1]
# Read MERRA-2 daily file
ex.nc	           <- open.nc(nc_file)
MERRA2_Lat       <- var.get.nc(ex.nc,"lat")
MERRA2_Lon       <- var.get.nc(ex.nc,"lon")
MERRA2_TLML      <- var.get.nc(ex.nc,"TLML") # surface_air_temperature [K]
close.nc(ex.nc)
# Define arries
Pixel_TLML       <- array(NA,dim=c(Nlines))
Pixel_Group      <- array(NA,dim=c(Nlines))
}else{
print(paste("!ERROR, Check files at:",date))
next
}
# Compute GC-informed surface number desnity from OMI
for(line in 1:Nlines){
#if( (line - (line%/%1000)*1000) == 0 ){
#  print(line/Nlines*100)
#}
# Get the row of CCD
Pixel_Row               <- as.numeric(merge_data[line,3])
# Only use rows that still vaild at the 2019
if( is.na(match(Pixel_Row,Rows_valid)) ){
# Pixel has a row that is not in Rows_valid
Pixel_TLML[line]      <- NA
Pixel_Group[line]     <- NA # Out of range
}else{
Pixel_LatCenter       <- as.numeric(merge_data[line,14])
Pixel_LonCenter       <- as.numeric(merge_data[line,19])
Pixel_LatConcers      <- as.numeric(merge_data[line,10:13])
Pixel_LonConcers      <- as.numeric(merge_data[line,15:18])
Pixel_Hour            <- as.numeric(merge_data[line,7])
# Skip bad pixels
if( is.character(c(Pixel_LatCenter,Pixel_LonCenter,Pixel_LatConcers,Pixel_LonConcers,Pixel_Hour)) ){
Pixel_TLML[line]    <- NA
Pixel_Group[line]   <- NA # Out of range
}else{
row_MERRA2          <- which(abs(Pixel_LatCenter-MERRA2_Lat)==min(abs(Pixel_LatCenter-MERRA2_Lat)))[1]
col_MERRA2          <- which(abs(Pixel_LonCenter-MERRA2_Lon)==min(abs(Pixel_LonCenter-MERRA2_Lon)))[1]
hour_MERRA2         <- Pixel_Hour + 1
Pixel_TLML[line]    <- MERRA2_TLML[col_MERRA2,row_MERRA2,hour_MERRA2]    # [K]
if( Pixel_TLML[line] >= Temp_range[2] || Pixel_TLML[line] <= Temp_range[1] ){
Pixel_Group[line] <- NA # Out of range
}else{
Pixel_Group[line] <- which(abs(Pixel_TLML[line]-Temp_groups)==min(abs(Pixel_TLML[line]-Temp_groups)))[1]
}
}
}
}
#---> Now loop Temp groups
for(igroup in 1:NGroups){
print(igroup)
group_list           <- which(Pixel_Group==igroup)
if(length(group_list)>0){
merge_data_new     <- cbind(merge_data[group_list,c(1:19,31:32)],Pixel_TLML[group_list])
#---> Format the arraies
Data_temp          <- data.frame( format(merge_data_new, digits = 8, scientific = TRUE) )
#---> Save the data to file
Output_Name        <- paste(merge_T_dir,"merge_group_",I2C(igroup),"_",date,"_",
"T_",as.character(Temp_groups[igroup]),sep="")
write.fwf(x=Data_temp,file=Output_Name, width=rep(16,22),colnames=FALSE)
}
}
#---> Clean up
rm(merge_data,merge_data_new,Data_temp,Pixel_TLML,Pixel_Group,MERRA2_TLML)
} # End of daily loop
#---> Allocate the threads for this job
Cluster            <- makeCluster(NThreads)
registerDoParallel(Cluster)
#---> Use foreach for parrallel computing
foreach(iday = 1:NDates, .inorder = FALSE, .packages=c('RNetCDF','gdata')) %dopar% {
# Call daily merge function
Daily_merge(iday)
# Process printed
Log("Processed %s | %d of %d | percent %7.3f", Date_list[iday], iday, NDates, iday/NDates*100)
}
#---> Disallocate the threads
stopCluster(Cluster)
#---> Libraries and source code
library(doParallel)
source("/home/lei/R/TOOLS.R")
#---> Merge file dir
merge_dir          <- "/data/OMI_v3.0/merge/"
merge_T_dir        <- "/data/OMI_v3.0/merge_T/"
MERRA2_dir         <- "/data/MERRA2/"
#---> Set temperature groups
Temp_range         <- c(278,323) # [K]
Temp_res           <- 0.5
Temp_groups        <- seq(Temp_range[1]+0.5*Temp_res,Temp_range[2]-0.5*Temp_res,by=Temp_res)
NGroups            <- length(Temp_groups)
#---> Set valid rows that are still valid at the 2019
# Deal with OMI row anomalies
Rows_valid         <- c(seq(1,20),seq(55,60))
#---> Set date range of interest
Date_range         <- c("2010-01-01", "2018-06-30")
Date_list          <- seq(as.Date(Date_range[1]),as.Date(Date_range[2]),by="days")
NDates             <- length(Date_list)
#---> Set number of threads for parallel computing
NThreads           <- 75
#---> Set log messages printed in the terminal
# Run "nc -l 4000" before this
log.socket         <- make.socket(port=4000)
Log                <- function(text, ...) {
msg              <- sprintf(paste0(as.character(Sys.time()), " ", text, "\n"), ...)
cat(msg)
write.socket(log.socket, msg)
}
#---> Define the daily merge function
Daily_merge        <- function(iday){
date               <- paste(substr(Date_list[iday],1,4),substr(Date_list[iday],6,7),
substr(Date_list[iday],9,10),sep="")
# Daily merge file
# 3i8,6f8.1,19f15.6,4E15.6
# ID,j,i,TimeUTC(1:6,j),CornerLat(i,j),CornerLat(i,j+1),  &
# CornerLat(i+1,j),CornerLat(i+1,j+1),Lat(i,j),           &
# CornerLon(i,j),CornerLon(i,j+1),CornerLon(i+1,j),       &
# CornerLon(i+1,j+1),Lon(i,j),SAA(i,j),SZA(i,j),VAA(i,j), &
# VZA(i,j),AMFCF(i,j),AMFCP(i,j),AMF(i,j),AMFg(i,j),      &
# Albe(i,j),Column(i,j),ColumnDest(i,j),                  &
# ColumnUnc(i,j),RefCorVCD(i,j)
merge_file         <- paste(merge_dir,"merge_",date,"_new",sep="")
# MERRA-2 daily file
nc_file1           <- paste(MERRA2_dir,"MERRA2_300.tavg1_2d_flx_Nx.",date,".nc4.nc",sep="")
nc_file2           <- paste(MERRA2_dir,"MERRA2_400.tavg1_2d_flx_Nx.",date,".nc4.nc",sep="")
if(file.exists(nc_file1)){
nc_file          <- nc_file1
}
if(file.exists(nc_file2)){
nc_file          <- nc_file2
}
# Check both files
if( file.exists(merge_file) && file.exists(nc_file) ){
#print(paste("  - Process:",date))
# Read  merge file
merge_data       <- read.table(merge_file)
Nlines           <- dim(merge_data)[1]
# Read MERRA-2 daily file
ex.nc	           <- open.nc(nc_file)
MERRA2_Lat       <- var.get.nc(ex.nc,"lat")
MERRA2_Lon       <- var.get.nc(ex.nc,"lon")
MERRA2_TLML      <- var.get.nc(ex.nc,"TLML") # surface_air_temperature [K]
close.nc(ex.nc)
# Define arries
Pixel_TLML       <- array(NA,dim=c(Nlines))
Pixel_Group      <- array(NA,dim=c(Nlines))
}else{
print(paste("!ERROR, Check files at:",date))
next
}
# Compute GC-informed surface number desnity from OMI
for(line in 1:Nlines){
#if( (line - (line%/%1000)*1000) == 0 ){
#  print(line/Nlines*100)
#}
# Get the row of CCD
Pixel_Row               <- as.numeric(merge_data[line,3])
# Only use rows that still vaild at the 2019
if( is.na(match(Pixel_Row,Rows_valid)) ){
# Pixel has a row that is not in Rows_valid
Pixel_TLML[line]      <- NA
Pixel_Group[line]     <- NA # Out of range
}else{
Pixel_LatCenter       <- as.numeric(merge_data[line,14])
Pixel_LonCenter       <- as.numeric(merge_data[line,19])
Pixel_LatConcers      <- as.numeric(merge_data[line,10:13])
Pixel_LonConcers      <- as.numeric(merge_data[line,15:18])
Pixel_Hour            <- as.numeric(merge_data[line,7])
# Skip bad pixels
if( is.character(c(Pixel_LatCenter,Pixel_LonCenter,Pixel_LatConcers,Pixel_LonConcers,Pixel_Hour)) ){
Pixel_TLML[line]    <- NA
Pixel_Group[line]   <- NA # Out of range
}else{
row_MERRA2          <- which(abs(Pixel_LatCenter-MERRA2_Lat)==min(abs(Pixel_LatCenter-MERRA2_Lat)))[1]
col_MERRA2          <- which(abs(Pixel_LonCenter-MERRA2_Lon)==min(abs(Pixel_LonCenter-MERRA2_Lon)))[1]
hour_MERRA2         <- Pixel_Hour + 1
Pixel_TLML[line]    <- MERRA2_TLML[col_MERRA2,row_MERRA2,hour_MERRA2]    # [K]
if( Pixel_TLML[line] >= Temp_range[2] || Pixel_TLML[line] <= Temp_range[1] ){
Pixel_Group[line] <- NA # Out of range
}else{
Pixel_Group[line] <- which(abs(Pixel_TLML[line]-Temp_groups)==min(abs(Pixel_TLML[line]-Temp_groups)))[1]
}
}
}
}
#---> Now loop Temp groups
for(igroup in 1:NGroups){
print(igroup)
group_list           <- which(Pixel_Group==igroup)
if(length(group_list)>0){
merge_data_new     <- cbind(merge_data[group_list,c(1:19,31:32)],Pixel_TLML[group_list])
#---> Format the arraies
Data_temp          <- data.frame( format(merge_data_new, digits = 8, scientific = TRUE) )
#---> Save the data to file
Output_Name        <- paste(merge_T_dir,"merge_group_",I2C(igroup),"_",date,"_",
"T_",as.character(Temp_groups[igroup]),sep="")
write.fwf(x=Data_temp,file=Output_Name, width=rep(16,22),colnames=FALSE)
}
}
#---> Clean up
rm(merge_data,merge_data_new,Data_temp,Pixel_TLML,Pixel_Group,MERRA2_TLML)
} # End of daily loop
#---> Allocate the threads for this job
Cluster            <- makeCluster(NThreads)
registerDoParallel(Cluster)
#---> Use foreach for parrallel computing
foreach(iday = 1:NDates, .inorder = FALSE, .packages=c('RNetCDF','gdata')) %dopar% {
# Call daily merge function
Daily_merge(iday)
# Process printed
Log("Processed %s | %d of %d | percent %7.3f", Date_list[iday], iday, NDates, iday/NDates*100)
}
#---> Disallocate the threads
stopCluster(Cluster)
rmarkdown::render_site()
rmarkdown::render_site()
# Read GC satellite file
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.SpeciesConc.20170103_0830z.nc4")
#---> Load libraies
library(RNetCDF)
# Read GC satellite file
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.SpeciesConc.20170103_0830z.nc4")
print.nc(exex.nc)
print.nc(ex.nc)
HCHO           <- var.get.nc(ex.nc,"SpeciesConc_CH2O")
dim(HCHO)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.StateMet_avg.20170103_0830z.nc4")
print.nc(ex.nc)
Temp           <- var.get.nc(ex.nc,"Met_T")
dim(Temp)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.Aerosols.20170103_0830z.nc4")
print.nc(ex.nc)
AODDust        <- var.get.nc(ex.nc,"AODDust")
close.nc(ex.nc)
dim(AODDust)
# Read GC satellite file
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.SpeciesConc.20170103_0830z.nc4")
print.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.StateMet_avg.20170101_1200z.nc4")
Temp           <- var.get.nc(ex.nc,"Met_T")
close.nc(ex.nc)
dim(Temp)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.StateMet_avg.20170103_0830z.nc4")
Temp           <- var.get.nc(ex.nc,"Met_T")
dim(Temp)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.Aerosols.20170103_0830z.nc4")
AODDust        <- var.get.nc(ex.nc,"AODDust")
dim(AODDust)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.Aerosols.20170101_1200z.nc4")
AODDust        <- var.get.nc(ex.nc,"AODDust")
dim(AODDust)
close.nc(ex.nc)
# Read GC satellite file
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.SpeciesConc.20170103_2330z.nc4")
HCHO           <- var.get.nc(ex.nc,"SpeciesConc_CH2O")
dim(HCHO)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.SpeciesConc.20170101_1200z.nc4")
HCHO           <- var.get.nc(ex.nc,"SpeciesConc_CH2O")
dim(HCHO)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.StateMet_avg.20170103_0830z.nc4")
Temp           <- var.get.nc(ex.nc,"Met_T")
dim(Temp)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.StateMet_avg.20170101_1200z.nc4")
Temp           <- var.get.nc(ex.nc,"Met_T")
dim(Temp)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.Aerosols.20170103_0830z.nc4")
AODDust        <- var.get.nc(ex.nc,"AODDust")
dim(AODDust)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.Aerosols.20170101_1200z.nc4")
AODDust        <- var.get.nc(ex.nc,"AODDust")
dim(AODDust)
close.nc(ex.nc)
ex.nc	         <- open.nc("/home/lei/Desktop/GCHP.SpeciesConc.20170103_2330z.nc4")
HCHO           <- var.get.nc(ex.nc,"SpeciesConc_CH2O")
dim(HCHO)
print.nc(ex.nc)
GC_lat         <- var.get.nc(ex.nc,"lat")
GC_lon         <- var.get.nc(ex.nc,"lon")
dim(HCHO)
close.nc(ex.nc)
GC_lat
GC_lon
dim(HCHO)
length(GC_lat)
dim(HCHO)
image.plot(GC_lon, GC_lat,HCHO[,,1],
horizontal=T,legend.shrink=1,axis.args = list(cex.axis =1,padj=-1.75,tck=0.2),
legend.width=1,legend.mar=1,
legend.args=list(text=expression(paste("log(HCHO fire emission) [log(g/m2/hr)]",sep="")),padj=0.15,cex=1),
xlab='',ylab='',midpoint=T,axes=F,ann=F)
title(xlab="",cex.lab=1.25,font.lab=2)
GC_lon
GC_lat
image.plot(GC_lon, GC_lat,HCHO[,,1],
horizontal=T,legend.shrink=1,axis.args = list(cex.axis =1,padj=-1.75,tck=0.2),
legend.width=1,legend.mar=1,
legend.args=list(text=expression(paste("log(HCHO fire emission) [log(g/m2/hr)]",sep="")),padj=0.15,cex=1),
xlab='',ylab='',midpoint=T,axes=F,ann=F)
title(xlab="",cex.lab=1.25,font.lab=2)
#---> Load libraies
library(RNetCDF);library(fields); library(maps)
image.plot(GC_lon, GC_lat,HCHO[,,1],
horizontal=T,legend.shrink=1,axis.args = list(cex.axis =1,padj=-1.75,tck=0.2),
legend.width=1,legend.mar=1,
legend.args=list(text=expression(paste("log(HCHO fire emission) [log(g/m2/hr)]",sep="")),padj=0.15,cex=1),
xlab='',ylab='',midpoint=T,axes=F,ann=F)
title(xlab="",cex.lab=1.25,font.lab=2)
title(xlab="",cex.lab=1.25,font.lab=2)
axis(1,at=pretty(GC_lon),tck=0.015,lwd=1.5,cex.axis=1,font=1,padj=-1.5)
axis(4,at=pretty(GC_lat),tck=0.015,lwd=1.5,labels=F)
axis(2,at=pretty(GC_lat),tck=0.015,lwd=1.5,labels=F)
#///////////////////////////////////////////////////////////////////////////////////////////
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#                                 Main script for validation                              ++
#                         Lei Zhu (leizhu@fas.harvard.edu), 01/29/2019                    ++
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#///////////////////////////////////////////////////////////////////////////////////////////
#///////////////////////////////////////////////////////////////////////////////////////////
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#                                      STEP 1                                             ++
#                              Load library and tools                                     ++
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#///////////////////////////////////////////////////////////////////////////////////////////
#---> Load libraies
library(fields); library(rhdf5); library(SDMTools); library(RNetCDF); library(lubridate)
library(akima)
library(fields);
library(SDMTools);
library(RNetCDF);
library(lubridate)
library(akima)
library(rhdf5);
install.packages("BiocManager")
BiocManager::install("rhdf5")
getwd()
setwd(/data/ese5023)
setwd("/data/ese5023/")
rmarkdown::render_site()
