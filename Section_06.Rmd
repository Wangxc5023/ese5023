---
title: "Section 06 Statistical sleuth: Basics"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

> "Statistics is the grammar of science." - Karl Pearson

***

*The notes below are modified from the freely available online book [Applied Statistics with R](https://daviddalpiaz.github.io/appliedstats/applied_statistics.pdf).*

***

# Summary Statistics

R has built-in functions for a large number of summary statistics. For numeric variables, we can summarize data with the center and spread. We have covered many of the summary functions, make sure you understand what the following functions return: `mean()`, `median()`, `range()`, `max()`, `min()`, `sd()`, `var()`, `IQR()`, `summary()`. Pay attention to data sets containing `NA` values.

Recall the `Keeling_Data`, call the above functions:

```{r}
library(dplyr)
Keeling_Data     <- read.csv(file = "co2_mm_mlo.csv", header = T)
Keeling_Data_tbl <- as_tibble(Keeling_Data)

# Add a new column to the original tibble
Keeling_Data_tbl <- Keeling_Data_tbl %>% 
    mutate(co2_new = ifelse(quality==1, co2, NA))

# Notice we use pull() to get a vector from a tibble
Month_CO2 <- Keeling_Data_tbl %>% 
  pull(co2_new)

# Summary and descriptive statistics
mean(Month_CO2, na.rm=T)
median(Month_CO2, na.rm=T)
range(Month_CO2, na.rm=T)
sd(Month_CO2, na.rm=T)
var(Month_CO2, na.rm=T)
IQR(Month_CO2, na.rm=T)
summary(Month_CO2, na.rm=T)
```
***

# Quick plots

Now we have some data to work with, and we have learned about the data at the most basic level, our next task is to visualize the data. Often, a proper visualization can illuminate features of the data that can inform further analysis. We will look at four methods of visualizing data.

## Histograms

When visualizing a single numerical variable, a histogram will be our go-to tool, which can be created in R using the `hist()` function.

```{r}
hist(Month_CO2)
```

The histogram function has a number of parameters which can be changed to make our plot look much nicer. Use the `?` operator to read the documentation for the `hist()` to see a full list of these parameters.

```{r}
hist(Month_CO2,
     xlab = "Monthly CO2 mixing ratios (ppm)",
     main = "Histogram of Monthly CO2",
     breaks = 20,
     col = "blue",
     border = "red")
box(lwd=2,col="green")
```
By default R will attempt to intelligently guess a good number of `breaks`, but as we can see here,
it is sometimes useful to modify this yourself.

## Barplots

Somewhat similar to a histogram, a barplot can provide a visual summary of a **categorical variable**, or a numeric variable with a finite number of values, like a ranking from 1 to 10.

```{r}
barplot(table(Keeling_Data_tbl$month),
xlab = "Month",
ylab = "Frequency",
main = "Number of observations",
col = "dodgerblue",
border = "darkorange")
```

## Boxplots

To visualize the relationship between a numerical and categorical variable, we will use a boxplot. In the `Keeling_Data_tbl` dataset, the `month` is a categorical variable. First, note that we can use a single boxplot as an alternative to a histogram for visualizing a single numerical variable. To do so in R, we use the `boxplot()` function.

```{r}
boxplot(Month_CO2)
```

Or try to plot the raw data:

```{r}
boxplot(Keeling_Data$co2)
```

However, more often, we will use boxplots to compare a numerical variable for different values of a categorical variable.

```{r}
boxplot(co2_new ~ month, data=Keeling_Data_tbl)
```

Here we used the `boxplot()` command to create side-by-side boxplots. However, since we are now dealing with two variables, the syntax has changed. The R syntax `co2_new ~ month, data = Keeling_Data_tbl` reads “Plot the `co2_new` variable against the `month` variable using the `Keeling_Data_tbl` dataset.” We see the use of a `~` (which specifies a formula) and also a `data = argument`. This will be a syntax that is common to many functions we will use later.

```{r}
boxplot(co2_new ~ month, data=Keeling_Data_tbl,
xlab = "Month",
ylab = "CO2 (ppm)",
main = "Monthly CO2",
cex = 2,
col = "orange",
border = "darkgreen")
```

Again, `boxplot()` has a number of additional arguments which have the ability to make our plot more visually appealing.

## Scatterplots

Lastly, to visualize the relationship between two numeric variables, we will use a scatterplot. This can be done with the `plot()` function and the `~` syntax we just used with a boxplot. (The function plot() can also be used more generally; see the documentation for details.)

```{r}
plot(co2_new ~ year, data=Keeling_Data_tbl)
```

```{r}
plot(co2_new ~ year, data=Keeling_Data_tbl,
xlab = "Year",
ylab = "CO2 (ppm)",
main = "CO2 vs Year",
pch = "+",
cex = 2,
col = "navy")
```

# Distributions in R

When working with different statistical distributions, we often want to make probabilistic statements based on the distribution.

We typically want to know one of four things:

* The density (pdf) at a particular value.
* The distribution (cdf) at a particular value.
* The quantile value corresponding to a particular probability.
* A random draw of values from a particular distribution.

R has functions for obtaining density, distribution, quantile and random values.

The general naming structure of the relevant R functions is:
* `dname` calculates density (pdf) at input x.
* `pname` calculates distribution (cdf) at input x.
* `qname` calculates the quantile at an input probability.
* `rname` generates a random draw from a particular distribution.

Note that `name` represents the `name` of the given distribution. For example, consider a random variable `X` which is $N(\mu=2, \sigma^{2}=25)$. To calculate the value of the pdf at `x = 3`, that is, the height of the curve at `x = 3`, use: 

```{r}
dnorm(x = 3, mean = 2, sd = 5)
```

To calculate the value of the cdf at `x = 3`, that is, $P(X<=3)$, the probability that `X` is less than or equal to 3, use:

```{r}
pnorm(q = 3, mean = 2, sd = 5)
```

Or, to calculate the quantile for probability 0.975, use:
```{r}
qnorm(p = 0.975, mean = 2, sd = 5)
```

Lastly, to generate a random sample of size `n = 5`, use:
```{r}
rnorm(n = 5, mean = 2, sd = 5)
```

These functions exist for many other distributions, including but not limited to: 

* `binom` for `Binomial` distribution
* `t` for `t` distribution
* `pois` for `Poisson` distribution
* `f` for `F` distribution
* `chisq` for `Chi-Squared` distribution

Notice each distribution will have its own set of parameters which need to be passed to the functions as arguments.

# Models and tests

## Correlation

Correlation describes how closely related two variables are. *Pearson’s test* assumes your data is normally distributed and measures linear correlation. *Spearman’s test* does not assume normality and measures non-linear correlation. And *Kendall’s test* also does not assume normality and measures non-linear correlation, and is a more robust test - but it is harder to compute by hand, and as such is less commonly seen.

In R, `cor(Vector1, Vector2)` provides correlation coefficients, while `cor.test(Vector1, Vector2)` performs the statistical test, giving test statistics, p values, and other outputs. Both perform the Pearson test by default, but can be changed by providing the argument `method = "spearman"` or `method = "kendall"`.

Let's generate two random samples, both with a size of `n = 50` from the $N(\mu=0, \sigma^{2}=25)$ distribution, and use different tests:

```{r}
Sample1 <- rnorm(n = 50, mean = 0, sd = 5)
Sample2 <- rnorm(n = 50, mean = 0, sd = 5)

# perform the Pearson test by default
cor(Sample1, Sample2)
cor(Sample1, Sample2, method = "pearson")
cor(Sample1, Sample2, method = "spearman")
cor(Sample1, Sample2, method = "kendall")

cor.test(Sample1, Sample2)
```

## t-test

The t-test is a method of comparing the mean(s). For one-sample t-test, the *null hypothesis* is the sample mean equals to a fixed value. For example, let's generate a random sample of size `n = 50` from $N(\mu=0, \sigma^{2}=25)$, and use t-test to check whether the mean is sigificantly different from zero.

```{r}
Sample <- rnorm(n = 50, mean = 0, sd = 5)
t.test(Sample, mu=0)
```

Two-sample t-test is almost the same as one-sample t-Test, but now the *null hypothesis* changes. Let's generate two random samples, both with a size of `n = 50` from the $N(\mu=0, \sigma^{2}=25)$ distribution, and use t-test to check whether the means are sigificantly different.

```{r}
Sample1 <- rnorm(n = 50, mean = 0, sd = 5)
Sample2 <- rnorm(n = 50, mean = 0, sd = 5)

t.test(Sample1, Sample2)
```

You can use `t.test()` to test independent or paired samples, use `?` to see more.

# Simple linear regression

This is most used model in all fields - simple but powerful. We first fit the model using `lm()` then use `summary()` to view the results in greater detail.

```{r}
x <- rnorm(n = 50, mean = 0, sd = 5)
a <- rnorm(n = 50, mean = 1, sd = 0.5)
b <- rnorm(n = 50, mean = 0, sd = 5)
y <- x * a + b
plot(x,y)
fit <- lm(y ~ x)
summary(fit)

```

You can extract the regression information directly with the `names()` function:
```{r}
names(summary(fit))
summary(fit)$coefficients
```

As you can see, the `names()` function tells us what information is available, and then we use
the `$` operator and coefficients to extract the information we are interested in. 

We can plot the regression line by:

```{r}
plot(y ~ x,
xlab = "x",
ylab = "y",
main = "y vs x",
pch = 20,
cex = 2,
col = "grey")
abline(fit, lwd = 5, col = "red")
points(mean(x), mean(y), pch = "+", cex = 3)
```

***

# In-class exercises

## Exercise #1

Please go over the notes once again, make sure you understand the scripts.

## Exercise #2

2.1 Use the two-sample t-test to determine how does the mean CO~2~ changes from 1958-1990 to 1991-2020. Report the mean change and 95% confident intervals.

2.2 Use the simple linear regression to compute the annual increase rate of CO~2~. Report the mean rate and 95% confident intervals.

## Exercise #3

One of the most important theorems in all of statistics is called the *Central Limit Theorem* or the *Law of Large Numbers*.  

It states that given a sufficiently large sample size from a population with a finite level of variance, the mean of all samples from the same population will be approximately equal to the mean of the original population. 

Furthermore, as you increase the number of samples and the sample size, the distribution of all of the sample means will approximate a normal distribution even if the original variables themselves are not normally distributed.

Can you demonstrate the two statements using R?

# Further reading

* [Applied Statistics with R](https://daviddalpiaz.github.io/appliedstats/applied_statistics.pdf)

* [Introduction to Data Exploration and Analysis with R](https://bookdown.org/mikemahoney218/LectureBook/basic-statistics-using-r.html)

* [r-statistics](https://r-statistics.co/)
